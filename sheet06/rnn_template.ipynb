{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sheet 6: Training a RNN to learn sinusoidal oscillations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as tc\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK 2: Set the hidden size (dimension of z_t)\n",
    "# Try different values: start with 2, then 4, 8, 16...\n",
    "# Find the minimum number that works well\n",
    "hidden_size = None  # TODO: Replace with your choice\n",
    "\n",
    "# Training parameters\n",
    "epochs = 500\n",
    "learning_rate = None # TODO: Try different values\n",
    "\n",
    "# TASK 4: Mini-batching parameters (uncomment and modify when needed)\n",
    "# seq_length = None     # Length of sampeld subsequences for training\n",
    "# batch_size = None     # Number of subsequences per batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% DATA LOADING AND VISUALIZATION\n",
    "\n",
    "# Load the sinusoidal data: x_t = [sin(t*π/10), cos(t*π/10)] for t=0,...,40\n",
    "data = tc.load('sinus.pt')\n",
    "observation_size = data.shape[1]  # Should be 2 (sin and cos components)\n",
    "\n",
    "print(f\"Data shape: {data.shape}\")\n",
    "print(f\"Observation size: {observation_size}\")\n",
    "print(f\"Time steps: {data.shape[0]}\")\n",
    "\n",
    "# Plot the input data\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(data[:, 0], label='sin(t*π/10)', linewidth=2)\n",
    "plt.plot(data[:, 1], label='cos(t*π/10)', linewidth=2)\n",
    "plt.xlabel('Time step')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Input Sinusoidal Data')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Plot phase space (sin vs cos)\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.plot(data[:, 0], data[:, 1], 'b-', linewidth=2, alpha=0.7)\n",
    "plt.scatter(data[0, 0], data[0, 1], c='green', s=100, label='Start', zorder=5)\n",
    "plt.scatter(data[-1, 0], data[-1, 1], c='red', s=100, label='End', zorder=5)\n",
    "plt.xlabel('sin(t*π/10)')\n",
    "plt.ylabel('cos(t*π/10)')\n",
    "plt.title('Phase Space Plot')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% MODEL DEFINITION\n",
    "\n",
    "class LatentRNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Recurrent Neural Network for learning dynamical systems\n",
    "    \n",
    "    Architecture:\n",
    "    z_t = tanh(C * x_{t-1} + W * z_{t-1} + h)  # Hidden state update\n",
    "    \\hat{x}_t = B * z_t + c                    # Output generation\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, obs_dim, latent_dim):\n",
    "        super(LatentRNN, self).__init__()\n",
    "        \n",
    "        self.obs_dim = obs_dim        # Dimension of observations (2 for sin/cos)\n",
    "        self.latent_dim = latent_dim  # Dimension of hidden state z_t\n",
    "        \n",
    "        # TASK 2: Implement the RNN layers here\n",
    "\n",
    "        # Option 1: Use nn.RNN (easier)\n",
    "        #or \n",
    "        # Option 2: Implement manually with nn.Linear layers (more educational)\n",
    "        # You need to define the following components:\n",
    "        # 1. Input-to-hidden transformation: U matrix and bias b\n",
    "        # 2. Hidden-to-hidden transformation: V matrix  \n",
    "        # 3. Hidden-to-output transformation: W matrix and bias c\n",
    "        # Hint: You can use nn.Linear layers or define nn.Parameter matrices\n",
    "        \n",
    "        # TODO: Implement the network architecture\n",
    "        \n",
    "        pass  # Remove this when you implement the layers\n",
    "        \n",
    "    def forward(self, time_series, h0):\n",
    "        \"\"\"\n",
    "        Forward pass through the RNN\n",
    "        \n",
    "        Args:\n",
    "            time_series: Input sequence of shape (seq_len, batch_size, obs_dim)\n",
    "            h0: Initial hidden state of shape (1, batch_size, latent_dim)\n",
    "            \n",
    "        Returns:\n",
    "            obs_output: Predicted observations of shape (seq_len, batch_size, obs_dim)\n",
    "            h: Final hidden state of shape (1, batch_size, latent_dim)\n",
    "        \"\"\"\n",
    "        \n",
    "        # TASK 2: Implement the forward pass\n",
    "        # The output should be predicted observations and the final hidden state\n",
    "\n",
    "        # Hint: For each time step, update hidden state and generate output\n",
    "        \n",
    "        # TODO: Implement the forward loop\n",
    "        \n",
    "        obs_output = None  # Replace with your implementation\n",
    "        h = None           # Replace with your implementation\n",
    "        \n",
    "        return obs_output, h\n",
    "    \n",
    "\n",
    "# Initialize the model\n",
    "model = LatentRNN(observation_size, hidden_size)\n",
    "# Print model information\n",
    "print(f\"\\nModel Architecture:\")\n",
    "print(f\"- Observation dimension: {observation_size}\")\n",
    "print(f\"- Hidden dimension: {hidden_size}\")\n",
    "print(f\"- Total parameters: {sum(p.numel() for p in model.parameters())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(learning_rate, moment=0, optimizer_function='SGD', print_loss=True, batch_size=1, batch_sequence_length=1):\n",
    "    \"\"\"\n",
    "    Training function with configurable optimizers and mini-batching\n",
    "    \n",
    "    Students need to implement the missing parts marked with TODO comments.\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO: Implement optimizer selection\n",
    "    # Create the appropriate optimizer based on optimizer_function parameter\n",
    "    # Available options: 'SGD' (with momentum support) and 'ADAM'\n",
    "    # Hint: Use optim.SGD() and optim.Adam() from torch.optim\n",
    "    \n",
    "    if optimizer_function == 'SGD':\n",
    "        # TODO: Initialize SGD optimizer with learning_rate and momentum\n",
    "        # optimizer = ...\n",
    "        pass\n",
    "    elif optimizer_function == 'ADAM':\n",
    "        # TODO: Initialize Adam optimizer with learning_rate\n",
    "        # optimizer = ...\n",
    "        pass\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown optimizer: {optimizer_function}\")\n",
    "    \n",
    "    # TODO: Define the loss function\n",
    "    # Use Mean Squared Error (MSE) loss for this regression task\n",
    "    # loss_function = ...\n",
    "    \n",
    "    losses = []\n",
    "    \n",
    "    print(f\"\\nStarting training for {epochs} epochs...\")\n",
    "    print(f\"Optimizer: {optimizer_function}, LR: {learning_rate}, Batch size: {batch_size}, Sequence length: {batch_sequence_length}\")\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # TODO: Initialize hidden state for the batch\n",
    "        # Create a tensor of shape (1, batch_size, hidden_size) with random values\n",
    "        # h0 = ...\n",
    "        \n",
    "        # Prepare full sequences (input and target)\n",
    "        x = data[:-1]  # Input: all timesteps except the last\n",
    "        y = data[1:]   # Target: all timesteps except the first\n",
    "        \n",
    "        # TODO: Create batch tensors for mini-batching\n",
    "        # Initialize tensors to hold batch data of shape (batch_sequence_length, batch_size, observation_dim):\n",
    "        # X = ...\n",
    "        # Y = ...\n",
    "        \n",
    "        # TODO: Implement mini-batching\n",
    "        # For each element in the batch, sample a random subsequence from the data\n",
    "        # The subsequence should start at a random index and have length batch_sequence_length\n",
    "        # Make sure the random index doesn't exceed the data bounds\n",
    "        \n",
    "        for j in range(batch_size):\n",
    "            # TODO: Sample a random starting index for the subsequence\n",
    "            # Ensure: 0 <= ind <= len(x) - batch_sequence_length\n",
    "            # ind = ...\n",
    "            \n",
    "            # TODO: Extract subsequence and assign to batch tensors\n",
    "            # X[j] = ...\n",
    "            # Y[j] = ...\n",
    "            pass\n",
    "        \n",
    "        # TODO: Forward pass\n",
    "        # 1. Zero the gradients from previous iteration\n",
    "        # 2. Run the model forward pass with input X and initial hidden state h0\n",
    "        # 3. Calculate the loss between model output and target Y\n",
    "        \n",
    "        # TODO: Backward pass and optimization step\n",
    "        # 1. Compute gradients via backpropagation\n",
    "        # 2. Update model parameters\n",
    "\n",
    "        \n",
    "        # Store loss for plotting\n",
    "        losses.append(epoch_loss.item())\n",
    "        \n",
    "        # Print progress\n",
    "        if epoch % 10 == 0 and print_loss:\n",
    "            print(f\"Epoch: {epoch} loss {epoch_loss.item():.6f}\")\n",
    "    \n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "losses = train() # TODO: input the right hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% MODEL EVALUATION AND PREDICTION\n",
    "\n",
    "# Generate predictions for 5 times the original sequence length\n",
    "prediction_length = 5 * data.shape[0]\n",
    "\n",
    "with tc.no_grad():\n",
    "    # Initialize hidden state and predictions tensor\n",
    "    # h = \n",
    "    # predictions = \n",
    "    \n",
    "    # Start with first data point\n",
    "    input_ = data[0:1].unsqueeze(1)  # Shape: (1, 1, obs_dim)\n",
    "    \n",
    "    # Generate sequence autoregressively, i.e. freely by providing the output as input\n",
    "    for i in range(prediction_length):\n",
    "        # TODO: make predictions and use prediction as input\n",
    "        #predictions[i] =  store predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% VISUALIZATION OF RESULTS"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
